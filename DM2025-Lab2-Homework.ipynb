{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "GitHub ID:\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code section is responsible for loading and splitting the dataset. We first parse the nested JSON structure from final_posts.json to extract the text content of each post. Then, emotion.csv is loaded to obtain emotion labels for the training samples, and data_identification.csv is used to determine whether each sample belongs to the training or test split. Using id as the merging key, the dataset is separated into train_df (containing both text and labels) and test_df (containing only text), which will serve as the input for subsequent feature engineering and model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´é›†æ•¸é‡: 47890, æ¸¬è©¦é›†æ•¸é‡: 16281\n",
      "                                                text emotion\n",
      "0  I bet there is an army of married couples who ...     joy\n",
      "1                         This could only end badly.    fear\n",
      "2  My sister squeezed a lime in her milk when she...     joy\n",
      "3                                Thank you so muchâ¤ï¸     joy\n",
      "4  Stinks because ive been in this program for a ...     joy\n"
     ]
    }
   ],
   "source": [
    "### 1. è³‡æ–™è®€å–èˆ‡åˆ†å‰²\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "DATA_ROOT = r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\"\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # 1. è®€å–èˆ‡è§£æåŸå§‹ JSON\n",
    "    with open(fr\"{DATA_ROOT}\\final_posts.json\", 'r', encoding='utf-8') as f:\n",
    "        posts_data = json.load(f)\n",
    "    \n",
    "    # è§£æå·¢ç‹€çµæ§‹ï¼Œæå–æ ¸å¿ƒè³‡æ–™\n",
    "    posts_list = [item['root']['_source']['post'] for item in posts_data]\n",
    "    posts_df = pd.DataFrame(posts_list)\n",
    "    \n",
    "    # 2. çµ±ä¸€æ¬„ä½åç¨± (post_id -> id)\n",
    "    posts_df.rename(columns={'post_id': 'id'}, inplace=True)\n",
    "\n",
    "    # 3. è®€å–è¼”åŠ© CSV æª”æ¡ˆ\n",
    "    emotion_df = pd.read_csv(fr\"{DATA_ROOT}\\emotion.csv\")\n",
    "    data_id_df = pd.read_csv(fr\"{DATA_ROOT}\\data_identification.csv\")\n",
    "\n",
    "    # 4. åˆä½µ split è³‡è¨Š (train/test æ¨™è¨˜)\n",
    "    merged_df = pd.merge(posts_df, data_id_df, on='id', how='left')\n",
    "    \n",
    "    # åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†\n",
    "    train_df = merged_df[merged_df['split'] == 'train'].copy()\n",
    "    test_df  = merged_df[merged_df['split'] == 'test'].copy()\n",
    "\n",
    "    # 5. å°‡æƒ…ç·’æ¨™ç±¤ (Label) åˆä½µå›è¨“ç·´é›†\n",
    "    train_df = pd.merge(train_df, emotion_df, on='id', how='left')\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# åŸ·è¡Œå‰è™•ç†\n",
    "train_df, test_df = load_and_preprocess_data()\n",
    "print(f\"è¨“ç·´é›†æ•¸é‡: {len(train_df)}, æ¸¬è©¦é›†æ•¸é‡: {len(test_df)}\")\n",
    "print(train_df[['text', 'emotion']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we first apply a customized simple_clean function to preprocess the text by converting it to lowercase, normalizing URLs and user mentions, and removing unnecessary symbols to reduce noise and improve semantic consistency. Then, we construct a TF-IDF vectorizer to transform the text into numerical features, incorporating unigrams and bigrams, stop-word removal, minimum document frequency, and sublinear scaling to capture meaningful tokens and word combinations relevant for emotion classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am sooo happy!!! visit URL joy USER\n"
     ]
    }
   ],
   "source": [
    "### 2. ç‰¹å¾µå·¥ç¨‹ï¼šæ–‡å­—å‰è™•ç† + TF-IDF åŸºæœ¬è¨­å®š\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def simple_clean(text):\n",
    "    \"\"\"ç°¡å–®æ–‡å­—å‰è™•ç†ï¼šå°å¯«ã€è™•ç†ç¶²å€/@/#ã€å»æ‰å¤šé¤˜ç¬¦è™Ÿ\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \" URL \", text)    # ç¶²å€ â†’ URL\n",
    "    text = re.sub(r\"@\\w+\", \" USER \", text)      # @user â†’ USER\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)       # #happy â†’ happy\n",
    "    text = re.sub(r\"[^\\w\\s!?]\", \" \", text)      # åªç•™å­—æ¯æ•¸å­—ç©ºç™½!?\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def build_tfidf_vectorizer():\n",
    "    \"\"\"å»ºç«‹ä¸€å€‹å¯é‡è¤‡ä½¿ç”¨çš„ TF-IDF å‘é‡å™¨\"\"\"\n",
    "    return TfidfVectorizer(\n",
    "        max_features=10000,        # æ¯”åŸæœ¬ 5000 å†å¤šä¸€äº›ç‰¹å¾µ\n",
    "        stop_words='english',      \n",
    "        ngram_range=(1, 2),        # uni + bi-gramï¼Œå°æƒ…ç·’è¾¨è­˜é€šå¸¸æœ‰å¹«åŠ©\n",
    "        min_df=2,                  # ä¸Ÿæ‰åªå‡ºç¾ä¸€æ¬¡çš„ noisy è©\n",
    "        sublinear_tf=True,         # ä½¿ç”¨ log(1 + tf)\n",
    "        preprocessor=simple_clean\n",
    "    )\n",
    "\n",
    "# æ¸¬è©¦ä¸€ä¸‹å‰è™•ç†\n",
    "print(simple_clean(\"I am sooo HAPPY!!! Visit http://xx.com #joy @user ğŸ˜‚\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model stage, we further split the training data into a training and validation set for parameter tuning and performance evaluation. We construct a Pipeline combining TF-IDF transformation and a LinearSVC classifier, and use GridSearchCV to explore different hyperparameter combinations, such as maximum features, n-gram scope, minimum document frequency, and the SVC regularization constant, using f1-macro as the evaluation metric for multi-class classification. After determining the best setup, the optimal model is retrained on the full training set and used to generate predictions for the test data, which are exported as a submission.csv file. \n",
    "### After uploading the results to the platform, the final test score achieved is 0.5696."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val split å®Œæˆï¼š\n",
      "Train: (38312,) Val: (9578,)\n",
      "é–‹å§‹ GridSearch èª¿åƒ...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "æœ€ä½³åƒæ•¸ï¼š\n",
      "{'clf__C': 0.5, 'tfidf__max_features': 10000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2)}\n",
      "cv ä¸Šçš„æœ€ä½³ f1_macroï¼š 0.40664534313043754\n",
      "\n",
      "=== Validation classification report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.54      0.50      0.52      2139\n",
      "     disgust       0.14      0.23      0.17       237\n",
      "        fear       0.30      0.49      0.38       402\n",
      "         joy       0.76      0.69      0.72      4759\n",
      "     sadness       0.33      0.38      0.35       785\n",
      "    surprise       0.35      0.37      0.36      1256\n",
      "\n",
      "    accuracy                           0.56      9578\n",
      "   macro avg       0.40      0.44      0.42      9578\n",
      "weighted avg       0.59      0.56      0.57      9578\n",
      "\n",
      "Validation f1_macro: 0.4159995495007635\n",
      "\n",
      "ä½¿ç”¨æœ€ä½³åƒæ•¸é‡è¨“å…¨è¨“ç·´é›†...\n",
      "æ­£åœ¨é æ¸¬æ¸¬è©¦é›†...\n",
      "æª”æ¡ˆå·²å„²å­˜: submission.csv\n"
     ]
    }
   ],
   "source": [
    "### 3. æ¨¡å‹ï¼šèª¿åƒ + è¨“ç·´ + é æ¸¬\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# 3-1. å…ˆåˆ‡å‡º train / validationï¼Œç”¨ä¾†èª¿åƒèˆ‡çœ‹æ¨¡å‹è¡¨ç¾\n",
    "X = train_df['text']\n",
    "y = train_df['emotion']\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train/Val split å®Œæˆï¼š\")\n",
    "print(\"Train:\", X_tr.shape, \"Val:\", X_val.shape)\n",
    "\n",
    "# 3-2. å»ºç«‹ Pipeline èˆ‡è¶…åƒæ•¸æœå°‹ç©ºé–“\n",
    "base_vectorizer = build_tfidf_vectorizer()\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('tfidf', base_vectorizer),\n",
    "    ('clf', LinearSVC(class_weight='balanced'))  # å°ä»˜é¡åˆ¥ä¸å¹³è¡¡\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [8000, 10000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__min_df': [1, 2],\n",
    "    'clf__C': [0.5, 1.0, 2.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',   # å¤šé¡åˆ¥é©åˆçš„åˆ†æ•¸\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"é–‹å§‹ GridSearch èª¿åƒ...\")\n",
    "grid.fit(X_tr, y_tr)\n",
    "\n",
    "print(\"\\næœ€ä½³åƒæ•¸ï¼š\")\n",
    "print(grid.best_params_)\n",
    "print(\"cv ä¸Šçš„æœ€ä½³ f1_macroï¼š\", grid.best_score_)\n",
    "\n",
    "# 3-3. çœ‹çœ‹åœ¨æˆ‘å€‘ä¿ç•™çš„ validation set ä¸Šçš„è¡¨ç¾\n",
    "best_model = grid.best_estimator_\n",
    "val_pred = best_model.predict(X_val)\n",
    "print(\"\\n=== Validation classification report ===\")\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(\"Validation f1_macro:\", f1_score(y_val, val_pred, average='macro'))\n",
    "\n",
    "# 3-4. ç”¨æœ€ä½³æ¨¡å‹è¨­å®šï¼Œé‡è¨“æ•´å€‹è¨“ç·´é›† (å…¨éƒ¨ train_df)\n",
    "print(\"\\nä½¿ç”¨æœ€ä½³åƒæ•¸é‡è¨“å…¨è¨“ç·´é›†...\")\n",
    "best_model.fit(train_df['text'], train_df['emotion'])\n",
    "\n",
    "# 3-5. é æ¸¬æ¸¬è©¦é›†ä¸¦ç”¢ç”Ÿ submission\n",
    "print(\"æ­£åœ¨é æ¸¬æ¸¬è©¦é›†...\")\n",
    "test_pred = best_model.predict(test_df['text'])\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'emotion': test_pred\n",
    "})\n",
    "\n",
    "# å°é½Š sample submission çš„ id é †åºå†å­˜æª”\n",
    "sample_sub_df = pd.read_csv(r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\\samplesubmission.csv\")\n",
    "submission = submission.set_index('id').reindex(sample_sub_df['id']).reset_index()\n",
    "submission.to_csv(r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\\submission.csv\", index=False)\n",
    "\n",
    "print(\"æª”æ¡ˆå·²å„²å­˜: submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the later implementation, I switched to using absolute file paths and cleaned up the output by only printing dataset sizes, making the initialization stage more concise and focused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¨“ç·´é›†æ•¸é‡: 47890, æ¸¬è©¦é›†æ•¸é‡: 16281\n"
     ]
    }
   ],
   "source": [
    "### 1. è³‡æ–™è®€å–èˆ‡åˆ†å‰²\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    # 1. è®€å–èˆ‡è§£æåŸå§‹ JSON\n",
    "    with open(r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\\final_posts.json\", 'r', encoding='utf-8') as f:\n",
    "        posts_data = json.load(f)\n",
    "    \n",
    "    # è§£æå·¢ç‹€çµæ§‹ï¼Œæå–æ ¸å¿ƒè³‡æ–™\n",
    "    posts_list = [item['root']['_source']['post'] for item in posts_data]\n",
    "    posts_df = pd.DataFrame(posts_list)\n",
    "    \n",
    "    # 2. çµ±ä¸€æ¬„ä½åç¨± (post_id -> id)\n",
    "    posts_df.rename(columns={'post_id': 'id'}, inplace=True)\n",
    "\n",
    "    # 3. è®€å–è¼”åŠ© CSV æª”æ¡ˆ\n",
    "    emotion_df = pd.read_csv(r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\\emotion.csv\")\n",
    "    data_id_df = pd.read_csv(r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\\data_identification.csv\")\n",
    "\n",
    "    # 4. åˆä½µ split è³‡è¨Š (train/test æ¨™è¨˜)\n",
    "    merged_df = pd.merge(posts_df, data_id_df, on='id', how='left')\n",
    "    \n",
    "    # åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†\n",
    "    train_df = merged_df[merged_df['split'] == 'train'].copy()\n",
    "    test_df = merged_df[merged_df['split'] == 'test'].copy()\n",
    "\n",
    "    # 5. å°‡æƒ…ç·’æ¨™ç±¤ (Label) åˆä½µå›è¨“ç·´é›†\n",
    "    train_df = pd.merge(train_df, emotion_df, on='id', how='left')\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "# åŸ·è¡Œå‰è™•ç†\n",
    "train_df, test_df = load_and_preprocess_data()\n",
    "print(f\"è¨“ç·´é›†æ•¸é‡: {len(train_df)}, æ¸¬è©¦é›†æ•¸é‡: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version, we directly instantiate a single TF-IDF vectorizer with fixed parameters to simplify the feature extraction process and improve training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. ç‰¹å¾µå·¥ç¨‹ï¼šå‰è™•ç† + TF-IDF è¨­å®š\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def simple_clean(text):\n",
    "    \"\"\"ç°¡å–®æ–‡å­—å‰è™•ç†ï¼šå°å¯«ã€è™•ç†ç¶²å€/@/#ã€å»æ‰å¤šé¤˜ç¬¦è™Ÿ\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \" URL \", text)    # ç¶²å€ â†’ URL\n",
    "    text = re.sub(r\"@\\w+\", \" USER \", text)      # @user â†’ USER\n",
    "    text = re.sub(r\"#(\\w+)\", r\"\\1\", text)       # #happy â†’ happy\n",
    "    text = re.sub(r\"[^\\w\\s!?]\", \" \", text)      # ç§»é™¤å¥‡æ€ªç¬¦è™Ÿï¼Œåªç•™å­—æ¯æ•¸å­—ç©ºç™½!? \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# TF-IDF ç‰¹å¾µå·¥ç¨‹ï¼ˆå¯ä»¥è¦–éœ€è¦èª¿æ•´è¶…åƒæ•¸ï¼‰\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,          # ç‰¹å¾µæ•¸é‡ä¸Šé™\n",
    "    stop_words='english',       # è‹±æ–‡åœç”¨è©\n",
    "    ngram_range=(1, 2),         # ä½¿ç”¨ unigram + bigram\n",
    "    min_df=2,                   # å‡ºç¾å¤ªå°‘çš„è©ä¸Ÿæ‰\n",
    "    sublinear_tf=True,          # ä½¿ç”¨ log(1 + tf)\n",
    "    preprocessor=simple_clean   # å¥—ç”¨è‡ªè¨‚å‰è™•ç†\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the model stage, I construct a Pipeline that integrates TF-IDF feature extraction and a LinearSVC classifier, and train the model using the labeled training data. After performing inference on the test set, we format the predictions into a submission.csv file according to the required submission format. This end-to-end pipeline efficiently transforms text into numerical features and performs emotion classification. \n",
    "### After uploading the results to the platform, the final test score achieved is 0.5685."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è¨“ç·´æ¨¡å‹ (LinearSVC)...\n",
      "æ­£åœ¨é æ¸¬æ¸¬è©¦é›†...\n",
      "æª”æ¡ˆå·²å„²å­˜: submission.csv\n"
     ]
    }
   ],
   "source": [
    "### 3. æ¨¡å‹ï¼šå»ºç«‹ Pipelineã€è¨“ç·´èˆ‡ç”¢ç”Ÿé æ¸¬æª”\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def train_and_predict(train_df, test_df, vectorizer):\n",
    "    # 1. å®šç¾© X (ç‰¹å¾µ) å’Œ y (ç›®æ¨™)\n",
    "    X_train = train_df['text']\n",
    "    y_train = train_df['emotion']\n",
    "    X_test = test_df['text']\n",
    "\n",
    "    # 2. å»ºç«‹ Pipeline (ä¸²æ¥ TF-IDF èˆ‡ LinearSVC)\n",
    "    model_pipeline = Pipeline([\n",
    "        ('tfidf', vectorizer),\n",
    "        ('clf', LinearSVC(C=1.0))   # ä¹‹å¾Œå¯ä»¥èª¿ C æˆ–åŠ  class_weight='balanced'\n",
    "    ])\n",
    "\n",
    "    # 3. è¨“ç·´æ¨¡å‹\n",
    "    print(\"æ­£åœ¨è¨“ç·´æ¨¡å‹ (LinearSVC)...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # 4. é€²è¡Œé æ¸¬\n",
    "    print(\"æ­£åœ¨é æ¸¬æ¸¬è©¦é›†...\")\n",
    "    predictions = model_pipeline.predict(X_test)\n",
    "\n",
    "    # 5. æ ¼å¼åŒ–è¼¸å‡º\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'emotion': predictions\n",
    "    })\n",
    "    \n",
    "    return model_pipeline, submission\n",
    "\n",
    "# åŸ·è¡Œæ¨¡å‹è¨“ç·´èˆ‡é æ¸¬\n",
    "model, submission = train_and_predict(train_df, test_df, tfidf_vectorizer)\n",
    "\n",
    "# 6. ç¢ºä¿é †åºä¸¦å­˜æª” (å°é½Š sample submission çš„ id é †åº)\n",
    "sample_sub_df = pd.read_csv(r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\\samplesubmission.csv\")\n",
    "submission = submission.set_index('id').reindex(sample_sub_df['id']).reset_index()\n",
    "submission.to_csv(r\"C:\\Users\\AN515\\Desktop\\DM2025-Lab2-Exercise\\kaggal_data\\submission.csv\", index=False)\n",
    "print(\"æª”æ¡ˆå·²å„²å­˜: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
